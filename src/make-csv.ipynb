{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import math\n",
    "import json\n",
    "\n",
    "rootdir = \"./\"  # Set Root folder directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "\n",
    "def get_advert_buffer(video_id, subdir, values):\n",
    "    if not values['Skippable']:\n",
    "        # Since Non Skippable ads will be completely buffered We can replace the buffer logs the duration to get 1/1 ratio later on.\n",
    "        return values['Duration']\n",
    "\n",
    "    filepath = os.path.join(subdir, 'AdvertBufferState.txt')\n",
    "    count = values['Count']\n",
    "    id_list = []\n",
    "    advert_buffer = []\n",
    "    id_list.append(video_id)\n",
    "    if count > 1:\n",
    "        for i in range(count, count+1):\n",
    "            temp = video_id+\"_\"+str(i)\n",
    "            id_list.append(temp)\n",
    "\n",
    "    stripped_dict = {}\n",
    "    with open(filepath) as data:\n",
    "        buffer_dict = json.loads(data.read())\n",
    "        for key, value in buffer_dict.items():\n",
    "            stripped_dict[key.strip()] = value\n",
    "\n",
    "    for vid_id in id_list:\n",
    "        buffer = stripped_dict[vid_id]['buffer']\n",
    "        ad_skip_duration = int(values['SkipDuration'])\n",
    "\n",
    "        ahead_buffer = [\n",
    "            ad_skip_duration, (ad_skip_duration+1), (ad_skip_duration+2), (ad_skip_duration+3)]\n",
    "        buffer_list = []\n",
    "        for val in ahead_buffer:\n",
    "            for datapoint in buffer:\n",
    "                current_buffer, video_player_in_seconds, resolution = datapoint\n",
    "                if float(video_player_in_seconds) < float(val):\n",
    "                    buffer_list.append(current_buffer)\n",
    "                else:\n",
    "                    # Read Buffer at skippable duration\n",
    "                    temp_val = float(current_buffer)\n",
    "                    temp_val += val  # Add spent buffer\n",
    "                    advert_buffer.append(temp_val)\n",
    "                    break  # To stop iterating over rest of list\n",
    "\n",
    "    return advert_buffer\n",
    "\n",
    "\n",
    "def get_buffer_index(buffer, timestamp):\n",
    "    for i in range(len(buffer)):\n",
    "        _, data_timestamp = buffer[i]\n",
    "        if data_timestamp == timestamp:\n",
    "            return i\n",
    "\n",
    "\n",
    "def get_lost_buffer(video_id, subdir, main_vid_duration):\n",
    "    '''\n",
    "    TODO: Check validity against a file that has ad count > 2\n",
    "    '''\n",
    "    wasted_buffer = []\n",
    "\n",
    "    filepath = os.path.join(subdir, 'BufferAdvert.txt')\n",
    "    with open(filepath) as f:\n",
    "        # Load List (This also works for list apparently)\n",
    "        data = json.loads(f.read())\n",
    "\n",
    "    filepath = os.path.join(subdir, 'buffer_details.txt')\n",
    "    with open(filepath) as f:\n",
    "        buffer = json.loads(f.read())  # Load the buffer into memory.\n",
    "\n",
    "    for datapoint in data:\n",
    "        dp_video_id, buff_info, main_timestamp = datapoint\n",
    "\n",
    "        if dp_video_id.strip() == video_id:\n",
    "            if (math.floor(abs(main_vid_duration-main_timestamp)) <= 1):  # Ad was at the end of the video\n",
    "                return -1\n",
    "\n",
    "            # Case where ad was at start of video\n",
    "            if type(buff_info) == float and float(buff_info) == float(0.0):\n",
    "                return -1\n",
    "            buff_size, timestamp = buff_info\n",
    "            datapoint_index_in_buffer = get_buffer_index(buffer, timestamp)\n",
    "            # try:\n",
    "            next_buffer_value, next_timestamp = buffer[datapoint_index_in_buffer+1]\n",
    "            inter_value = float(buff_size-next_buffer_value)\n",
    "            if inter_value < 0:\n",
    "                print(\"Negative Value of Buffer Lost: \",\n",
    "                      dp_video_id, inter_value, \"Folder: \", subdir)\n",
    "                inter_value = 0\n",
    "\n",
    "            wasted_buffer.append(inter_value)\n",
    "\n",
    "    if len(wasted_buffer) > 1:\n",
    "        # For ads that came multiple times. Add all the buffer that was wasted.\n",
    "        return np.array(wasted_buffer).sum()\n",
    "    else:\n",
    "        return wasted_buffer[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = []\n",
    "\n",
    "# For each Folder\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    new_dict = {}\n",
    "    dictionary_made = False\n",
    "    if subdir == rootdir:\n",
    "        continue\n",
    "\n",
    "    # Get path to stream details.\n",
    "    path = os.path.join(subdir, \"stream_details.txt\")\n",
    "    if not os.path.exists(path):\n",
    "        continue\n",
    "\n",
    "    # Open File and read\n",
    "    # print(\"Collecting buffer from path:\", path)\n",
    "    with open(path) as f:\n",
    "        data_json = json.loads(f.read())  # Load Dictionary\n",
    "\n",
    "    duration_path = os.path.join(subdir, \"BufferAdvert.txt\")\n",
    "    with open(duration_path) as f:\n",
    "        duration_json = json.loads(f.read())\n",
    "\n",
    "    main_vid_duration = data_json['Main_Video']['Total Duration']\n",
    "\n",
    "    for key, value in data_json.items():\n",
    "        if (key.strip() == \"empty_video\"):\n",
    "            continue\n",
    "\n",
    "        # If selected value is main_video, skip it as it doesn't require any processing.\n",
    "        if key.strip() == \"Main_Video\":\n",
    "            new_dict[\"Main_Video\"] = value\n",
    "            continue\n",
    "\n",
    "        print(\"Getting advert buffer for key:\", subdir, value)\n",
    "\n",
    "        if (value['SkipDuration'] != 999):\n",
    "            value['SkipDuration'] = 5\n",
    "\n",
    "        advert_buffers = get_advert_buffer(key.strip(), subdir, value)\n",
    "\n",
    "        lost_while_played = get_lost_buffer(\n",
    "            key.strip(), subdir, main_vid_duration)\n",
    "\n",
    "        if (type(advert_buffers) == list):\n",
    "            value['Advert_Buffer'] = advert_buffers[0]\n",
    "        else:\n",
    "            value['Advert_Buffer'] = advert_buffers\n",
    "\n",
    "        value['Seconds_Lost_To_Ad'] = lost_while_played\n",
    "        new_dict[key] = value\n",
    "\n",
    "    for index in range(len(duration_json)):\n",
    "        print(duration_json[index])\n",
    "        key = duration_json[index][0]\n",
    "\n",
    "        if (key.strip() == \"empty_video\"):\n",
    "            continue\n",
    "        print(key)\n",
    "        try:\n",
    "            new_dict[key]['Timestamp'] = duration_json[index][1][1]\n",
    "        except:\n",
    "            new_dict[key]['Timestamp'] = duration_json[index][2]\n",
    "\n",
    "    all_data.append(new_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making Rows for DataFrame\n",
    "max_number_of_ads = -1\n",
    "for i in range(len(all_data)):\n",
    "    for key, value in all_data[i].items():\n",
    "        if key == \"Main_Video\":\n",
    "            if value['UniqueAds'] > max_number_of_ads:\n",
    "                max_number_of_ads = value['UniqueAds']\n",
    "            # max_number_of_ads=value['Main_Video']['UniqueAds']\n",
    "\n",
    "column_names = [\"Main_Video_Url\", \"Unique_Ads\", \"Total Number of Ads\",\n",
    "                \"Duration(s)\", \"Size Original (Bytes)\", \"Resolution\"]\n",
    "add_columns = []\n",
    "\n",
    "print(all_data)\n",
    "\n",
    "for i in range(max_number_of_ads):\n",
    "    ad_id = f\"Advertisement {i+1} ID\"\n",
    "    add_columns.append(ad_id)\n",
    "    ad_count = f\"Advertisement {i+1} Count\"\n",
    "    add_columns.append(ad_count)\n",
    "    ad_skippable = f\"Advertisement {i+1} Skippable\"\n",
    "    add_columns.append(ad_skippable)\n",
    "    skip_duration = f\"Advertisement {i+1} Skip Duration\"\n",
    "    add_columns.append(skip_duration)\n",
    "    resolution = f\"Advertisement {i+1} Resolution\"\n",
    "    add_columns.append(resolution)\n",
    "    size = f\"Advertisement {i+1} Size(Bytes)\"\n",
    "    add_columns.append(size)\n",
    "    duration = f\"Advertisement {i+1} Durationn(s)\"\n",
    "    add_columns.append(duration)\n",
    "    advert_buffer = f\"Advertisement {i+1} Advert_Buffer(s)\"\n",
    "    add_columns.append(advert_buffer)\n",
    "    seconds_lost = f\"Advertisement {i+1} Main_Buffer_Lost(s)\"\n",
    "    add_columns.append(seconds_lost)\n",
    "    timestamp = f\"Avertisement {i+1} Timestamp\"\n",
    "    add_columns.append(timestamp)\n",
    "\n",
    "new_columns = column_names+add_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the rows\n",
    "all_data_rows = []\n",
    "for i in range(len(all_data)):\n",
    "    row = []\n",
    "    main_video_url = all_data[i]['Main_Video']['Url']\n",
    "    row.append(main_video_url)\n",
    "    unique_ads = all_data[i]['Main_Video']['UniqueAds']\n",
    "    row.append(unique_ads)\n",
    "\n",
    "    row.append(0)  # For Total Number of ads #Index 2\n",
    "\n",
    "    duration = all_data[i]['Main_Video']['Total Duration']\n",
    "    row.append(duration)\n",
    "    size = all_data[i][\"Main_Video\"][\"Size\"]\n",
    "    row.append(size)\n",
    "    # size_240p=all_data[i][\"Main_Video\"][\"Size240p\"]\n",
    "    # row.append(size_240p)\n",
    "    # size_1080p=all_data[i][\"Main_Video\"][\"Size720p\"]\n",
    "    # row.append(size_1080p)\n",
    "    res = all_data[i]['Main_Video']['Resolution']\n",
    "    row.append(res)\n",
    "    total_ads = 0\n",
    "    for key, value in all_data[i].items():\n",
    "        if key == \"Main_Video\":\n",
    "            continue\n",
    "        else:\n",
    "            row.append(key)\n",
    "            row.append(value['Count'])\n",
    "            total_ads = total_ads+value['Count']\n",
    "            row.append(value['Skippable'])\n",
    "            row.append(value['SkipDuration'])\n",
    "            row.append(value['Resolution'])\n",
    "            row.append(value['Size'])\n",
    "            # row.append(value['Size240p'])\n",
    "            # row.append(value['Size720p'])\n",
    "            row.append(value['Duration'])\n",
    "            row.append(value['Advert_Buffer'])\n",
    "            row.append(value['Seconds_Lost_To_Ad'])\n",
    "            row.append(value['Timestamp'])\n",
    "    row[2] = total_ads\n",
    "    all_data_rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(all_data_rows, columns=new_columns)\n",
    "df.to_csv('PakistanTrending.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
